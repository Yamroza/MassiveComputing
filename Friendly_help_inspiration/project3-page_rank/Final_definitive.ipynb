{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2264f219-b134-48e8-96f4-6e28acd97bc0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Steps to implement **Page Rank Algorithm**\n",
    "* Check Data Raw structure\n",
    "* Extract relevant data: [Document ID, List of links]\n",
    "* Transform *List of links* to *List od Docuemts ID*: *Forward Links Table*\n",
    "* Calculate *Number of output links*\n",
    "* Construct *Reverse Links Table* from *Forward Links Table*\n",
    "* Initialize *Page Rank Table*\n",
    "* Recalculate *Page Rank Table* until:\n",
    " * All the *Page Rank* values are stable\n",
    " * Reach number of iterations (sugested value: 20 iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b429d52e-a2c6-427b-aa94-bde7eb5325e8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# A) Library configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "000ae955-cd3b-4a52-9e0c-978241c38d5c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "10865609-2787-4474-900b-54e4dd830765",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.types import ArrayType, StringType,LongType, FloatType\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from operator import add "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7ae6143a-d173-4019-861b-e9955817c173",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"false\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dea502f-ce2d-4240-bfc7-237b98aedd3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 1) Check Data Raw Structure\n",
    "We will use Databrics Wikipedia dataset, which contains 2012 Wikipedia Database in english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5cfc6a76-106c-4ccd-a3d1-fa0c54a9eadb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we define the wikipediaDF Spark Dataframe, with the full database content:\n",
    "We need to know the total number of documents in this database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29455663-e769-4c76-a3b0-370d20b553a8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "wikipediaDF=spark.read.parquet(\"dbfs:/databricks-datasets/wikipedia-datasets/data-001/en_wikipedia/articles-only-parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d514c6bb-9ff1-4de1-8034-3f6d8df5f554",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To program the PageRank Algorithm, we need to extract a subset of the full database. We will select 0.00001 of the full database, and to avoid randomness behaviour, we set to a fixed seed value=0.\n",
    "\n",
    "**Note:** For the final evaluation, should change the fraction to 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f1bd9c8-7f2f-4780-b338-8e55fbd947a1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PartialWikipediaDF=wikipediaDF.sample(fraction=0.001,seed=0).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c5138c7d-9371-4cae-b2d4-3cdd9520d1e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we can check the data structure:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e2a16c2-ad6f-424e-9ea0-ad15ece5ae3e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Conclusions with the data raw analysis:\n",
    "* There are several columns, but the relevant information is stored in the follow columns:\n",
    " * **title**: The title of the document.\n",
    " * **id**: Id of the document\n",
    " * **text**: The content of the document. The most relevant information here (for the page rank algorithm) are the links to other documents. The link is enclosed in brackets, and contains the title of the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2ad68eec-cc51-419c-81db-27db4836f296",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 2) Extract Relevant Data\n",
    "From the previous conclusions, we knows we need to select just three columns [*title*,*id*,*text*], and the relevant information from the *text* column are the links, identified by the titles enclosed in brackets.\n",
    "\n",
    "Here we need to use regular expressions to select the relevant information from the *text* column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8fdf5ea6-644f-4384-97c7-4e32a40264b6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Here we will implement a *parse_links* function, who receive a string and return a list of strings with the titles of the pointed documents.\n",
    "\n",
    "This is a Python function, so is not direct callable from the Spark Dataframe, so, we need to define also the User Defined Function, to be usable in Spark Dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "77276afc-1f7c-4a23-860c-c923682a882e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parse_links(document_body):\n",
    "  document_body_u = document_body.lower() \n",
    "  data=re.findall(r'\\[\\[(?!category|wikipedia|file|help|special)((?:(?![\\[\\]]).)*)\\]\\]',document_body_u)\n",
    "  if (len(data)>0):\n",
    "    links=[s.split('|')[0].split('#')[0].lower() for s in data]\n",
    "  else:\n",
    "    links=[]\n",
    "  return links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above function, we used a regular expression to detect more complex Wikipedia links. This allows us to distinguish between compound links and add them to the list that will be used to calculate the forward links table later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "91652eea-ca38-4c8d-9291-7ad0f60fd66f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parse_links_udf = udf(parse_links,ArrayType(StringType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f0327a8-4d4b-497b-8a29-bedb4264256e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "It is necesarly convert the text to lowercase (both: *title* and *text* columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9c9ce40-6272-43bc-b370-eb7f6ee6ecce",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tolower_udf= udf(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5536767c-7153-4337-afe1-5c794540c4f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, we create parseDF with the selected information, renaming the result tables to \"title\", and \"links\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42ae3725-3c11-4190-b0e0-e220827c6361",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsedDF = PartialWikipediaDF.select(tolower_udf(\"title\").alias(\"title\"),\"id\",parse_links_udf(\"text\").alias(\"links\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "37a46987-b6cd-478e-91a8-ee0577de12aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 3) Transform *List of links* to *List od Docuemts ID*: *Forward Links Table*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6cfd450-2de2-462c-b565-e56e2714020c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "To get the *id* of the target documents, we need analyse the full Wikipedia Database and extract a table with this two information.\n",
    "\n",
    "This information is static, and used in distributed way. So, we will collect the data and convert to a Pandas Dataframe (PDF suffix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dbf50ad5-618a-4976-8aca-9bc7a8ac6fb2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titleidDF=wikipediaDF.select(\"id\",tolower_udf(\"title\").alias(\"title\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "65c08a41-e66f-4428-ae82-9a8130a6f980",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "titleidPDF=titleidDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e6e0245b-2b87-4fba-9094-3e37f5591374",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "broadcast_title_idPDF = sc.broadcast(titleidPDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1820edc1-5679-4e6d-8554-1cbc58633045",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def text_links_2idx_2(links):\n",
    "  title_idxPDF = broadcast_title_idPDF.value\n",
    "  if ( len(links)>0):\n",
    "  # This command looks in the title column if the elements in the list links exists, and if it exists\n",
    "  # gets his id value. The result is converted to a list.\n",
    "    result = title_idxPDF[title_idxPDF.title.isin(links)].id.to_list()\n",
    "  else:\n",
    "    result = [] \n",
    "  return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f27980ff-73a9-4c3c-bcbf-711577e64926",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "udf_text_links_2idx =udf(text_links_2idx_2,ArrayType(LongType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "138cb3fc-65c0-45df-84b6-ddcc69099d1c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ForwardDF = parsedDF.select(\"id\",udf_text_links_2idx(\"links\").alias(\"links\")).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "71c6e1f8-ff4d-4fe5-ad81-4710f2cc5f39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Once verified the function, we need to define the UDF function, to invoke it from the **parsedDF** to select just [id, list of ids].\n",
    "\n",
    "To be efficient, we can broadcast the variable *titleidPDF*, to call it in the transformation.\n",
    "\n",
    "We will call this Dataframe as: **ForwardDF**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f9c0997-5603-4aa4-a53f-361a04276882",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 4) Calculate *Number of output links*\n",
    "Using the **ForwardDF**, we need to calculate the number of output links per document. \n",
    "Because we will need this information to calculate the PageRank we will collect this information in a Pandas Dataframe, and define a Broadcast variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "87146027-ecdb-44ca-b59a-20635e1fbccf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ForwardDF_WC= ForwardDF.select(\"id\",\"links\",F.size(\"links\").alias(\"n_succesors\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a18b1740-9fa6-4f0f-9813-255da7e079fd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 5) Construct *Reverse Links Table* from *Forward Links Table*\n",
    "Now, we will define the Reverse Links Table Dataframe (**ReverseDF**), transforming the **ForwardDF** to a Dataframe with [*id*,*list of ids*] or similar.\n",
    "\n",
    "*Suggestion*: Maybe the *list of ids* could contains not only the id of the target document, also the number of output links. This will improve the Page Rank calcule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a6b2b39f-c2b2-411d-a50c-707a5b32d83a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def reverseId(id,links):\n",
    "  if (len(links)>0):\n",
    "    reverse = [ (tgt_id,id) for tgt_id in links ]\n",
    "  else:\n",
    "    reverse=[]\n",
    "  return reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e4be51a-71c7-44e7-b108-935c3100ff44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ForwardRDD = ForwardDF_WC.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d717b5d-f010-4b9f-91b7-155cf1af0136",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReverseRDD=(ForwardRDD\n",
    " .flatMap(lambda r: reverseId(r.id,r.links))\n",
    " .groupByKey()\n",
    " .map(lambda r: (r[0],list(r[1])))\n",
    " )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ab97e427-82e4-43a4-bf98-7dc3e8d6e8ba",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reverseDF=spark.createDataFrame(ReverseRDD,[\"id\",\"links\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1d0757f-c3cc-4736-9345-34c9bc9a5dfc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "reverseDF_WC= reverseDF.select(\"id\",\"links\",F.size(\"links\").alias(\"n_precessors\")).withColumnRenamed(\"links\",\"precessors\").withColumnRenamed(\"id\",\"id1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b624a91d-8ce6-4da5-b17e-737792768b17",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "joineddf = reverseDF_WC.join(ForwardDF_WC, ForwardDF_WC.id == reverseDF_WC.id1, how='full')\n",
    "YesId1DF = joineddf.filter(\"id1 is NOT NULL\")\n",
    "NoId1DF = joineddf.filter(\"id1 is NULL\")\n",
    "NoId1DF_v1 = NoId1DF.withColumn(\"id1\", NoId1DF.id)\n",
    "FullId1DF= YesId1DF.union(NoId1DF_v1)\n",
    "ReverseDF = FullId1DF.select(\"id1\", \"precessors\", \"n_precessors\", \"links\", \"n_succesors\").withColumnRenamed(\"links\",\"succesors\").withColumnRenamed(\"id1\",\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "357d2080-8b96-4bad-ab56-cc3fadd4430a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReverseDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "32debbf5-49fa-4c83-8d2c-02f61f903557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b6c19d4f-7ea5-43d5-a18c-d670ddc31613",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 6) Initialize *Page Rank Table*\n",
    "We define a Pandas DataFrame (**PageRankPDF**) with the ids of documents in the **ReverseDF**, with the initial value. \n",
    "\n",
    "This could be:\n",
    "\n",
    "\\\\(\\frac{0.85}{N}\\\\)\n",
    "\n",
    "where *N* is the number of documents in the **ReverseDF**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11a558dc-b43f-4998-a92b-1eb9921c5482",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReverseDF= ReverseDF.select(\"id\", \"precessors\", \"n_precessors\",\"n_succesors\").withColumn(\"rank\", F.lit(0.2)).na.fill(0).cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aef52e32-1fd9-44bc-b298-1e9e36e5156a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we should remove the repeated values from the predecessors of each list, otherwise, the algorithm can easily be overwhelmed and tricked by having a webpage that mentions itself several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5540af1d-683f-4a3e-abea-5abdbe61d5bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReversePDF= ReverseDF.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "43373c3c-7945-4422-a4dc-2c406d407d60",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "line = ReversePDF[ReversePDF[\"id\"]== 204228][\"precessors\"].tolist()[0]\n",
    "line[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7a864f52-aa5c-4c4a-a246-724249f7a74e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# 7) Recalculate *Page Rank Table* until:\n",
    " * All the *Page Rank* values are stable\n",
    " * Reach number of iterations (sugested value: 20 iterations)\n",
    "\n",
    "Should define a PageRank function, who receives the *id* of the document, *list of links*, and current PageRank table (**PageRankPDF**)*, and returns the new PageRank of the document *id* \n",
    "The loop must use the **ReverseDF** as master of information.\n",
    "\n",
    "Now, we define a loop:\n",
    "* While the conditions of exit are false:\n",
    " * Calculate the new page ranks, invoking the PageRank function from the **ReverseDF**, creating a new Dataframe NewPageRankDF with the [id,new_pagerank] info. (Could contains more information, if you use the suggestion in section 5) )\n",
    " * Collect the new page ranks and compares it with the previous **PageRankPDF**, checking if the new Page Ranks vary more than a threshold. If not, the exit condition is complied.\n",
    " * Update the **PageRankPDF** with the new values,and update the UDF functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18d9cdc5-a2de-4e75-9740-8a23bf738895",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def new_pagerank_wd(id1, pagerank, reversePDF):\n",
    "    '''\n",
    "    1 - Iterar por los diferentes ids de los preccessors \n",
    "    2 - Hacer el sumatorio ponderado (formula de PageRank) \n",
    "    '''\n",
    "    list_of_ids= reversePDF[reversePDF[\"id\"]== id1][\"precessors\"].tolist()[0]\n",
    "    d = 0.85 \n",
    "    \n",
    "    suma= 0.0\n",
    "    new_rank = 0.0\n",
    "    \n",
    "    if list_of_ids != None:     \n",
    "      no_repeated_list = list(set(list_of_ids))\n",
    "      for k in no_repeated_list:\n",
    "        a = int(k) \n",
    "        if int(id1) != a:\n",
    "          s= pagerank[pagerank[\"id\"]== a][\"n_succesors\"].tolist()\n",
    "          r= pagerank[pagerank[\"id\"]== a][\"rank\"].tolist()\n",
    "          if int(s[0]) != 0: \n",
    "            suma += float(r[0]/s[0])\n",
    "            \n",
    "      new_rank = (1-d)+ d * suma\n",
    "          \n",
    "    return float(new_rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5c631038-6258-4139-aad9-4332a2ae3194",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def new_pagerank(id1, pagerank, reversePDF):\n",
    "    '''\n",
    "    1 - Iterar por los diferentes ids de los preccessors \n",
    "    2 - Hacer el sumatorio ponderado (formula de PageRank) \n",
    "    '''\n",
    "    list_of_ids= reversePDF[reversePDF[\"id\"]== id1][\"precessors\"].tolist()[0]\n",
    "    \n",
    "    suma= 0.0\n",
    "    new_page_rank = 0.0\n",
    "    if list_of_ids != None:     \n",
    "      no_repeated_list = list(set(list_of_ids))\n",
    "      for k in no_repeated_list:\n",
    "        a = int(k) \n",
    "        temp= pagerank\n",
    "        if int(id1) != a:\n",
    "          s= pagerank[pagerank[\"id\"]== a][\"n_succesors\"].tolist()\n",
    "          r= pagerank[pagerank[\"id\"]== a][\"rank\"].tolist()\n",
    "          if int(s[0]) != 0:          \n",
    "            suma += float(r[0]/s[0]) \n",
    "       \n",
    "    return float(suma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db8a40b5-a37d-4c28-8623-e9a3088dce0d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def new_pagerank_nosuccessors(pagerank,size):\n",
    "    '''\n",
    "    Takes into account all the ids that do not have any succesor, and shares their rank with the whole dataframe  \n",
    "    '''\n",
    "    # First we add the corresponding value for the pages with no successors \n",
    "    withoutNextDF = pagerank.filter(pagerank[\"n_succesors\"]==0)\n",
    "    #temp= withoutNextDF.select(\"rank\").rdd.reduce(add)\n",
    "    #sum= 0.0\n",
    "    #for i in temp: sum += i \n",
    "    value = withoutNextDF.select(F.sum(withoutNextDF[\"rank\"]).alias(\"rank\")).collect()[0][\"rank\"]\n",
    "    \n",
    "    UnionDF = pagerank.withColumn(\"rank\", pagerank.rank + F.lit(value/size))\n",
    "    \n",
    "    # Then we will normalize the data set so that the greatest value of the rank will be 1 \n",
    "    max_val = UnionDF.agg({\"rank\" : \"max\"}).first()[0]\n",
    "    newpagerank= UnionDF.withColumn(\"rank\", UnionDF.rank/max_val)\n",
    "    \n",
    "    return newpagerank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "34e3f1b2-e229-427c-95e5-34067c87863d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "math.sqrt(9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cfa14d03-c420-4247-9a80-f9ce2e57653d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "count= 0\n",
    "temp= 2\n",
    "# broadcast_PageRankPDF= sc.broadcast(ReversePDF)\n",
    "PageRankPDF= ReversePDF\n",
    "size = ReversePDF.shape[0]\n",
    "PrevRankPDF= PageRankPDF\n",
    "temps1= []\n",
    "temps2= []\n",
    "temps3= []\n",
    "\n",
    "while (count < 20) and (temp > 0.001):\n",
    "    \n",
    "    # First we take into account the contribution of the usual pages\n",
    "    # udf_new_pagerank = udf(lambda l: new_pagerank(l, PageRankPDF), FloatType())\n",
    "    udf_new_pagerank = udf(lambda l: new_pagerank_wd(l, PageRankPDF, ReversePDF), FloatType())\n",
    "    # NewPageRankDF = ReverseDF.select(\"id\",udf_new_pagerank(\"precessors\").alias(\"rank\"),\"n_succesors\").cache()\n",
    "    NewPageRankDF = ReverseDF.select(\"id\", udf_new_pagerank(\"id\").alias(\"rank\"),\"n_succesors\").cache()\n",
    "    \n",
    "    # Then the contribution of the non successor pages \n",
    "    #NewPageRankDF_2 = new_pagerank_nosuccessors(NewPageRankDF,size)\n",
    "    #PageRankPDF = NewPageRankDF_2.toPandas() \n",
    "    PageRankPDF = NewPageRankDF.toPandas()\n",
    "    \n",
    "    # temp = abs(PageRankPDF[\"rank\"] - PrevRankPDF[\"rank\"]).sum()\n",
    "    temp1 = abs((abs(PageRankPDF[\"rank\"].sum()) - abs(PrevRankPDF[\"rank\"].sum()))/size) *1.0 \n",
    "    temps1.append([temp1]) \n",
    "    temp2 = abs((abs(PageRankPDF[\"rank\"].sum()) - abs(PrevRankPDF[\"rank\"].sum()))/math.sqrt(size)) *1.0 \n",
    "    temps2.append([temp2]) \n",
    "    temp3 = abs(((abs(PageRankPDF[\"rank\"])- abs(PrevRankPDF[\"rank\"]))/abs(PageRankPDF[\"rank\"])).sum())\n",
    "    temps3.append([temp3])\n",
    "    \n",
    "    PrevRankPDF = PageRankPDF\n",
    "    count += 1 \n",
    "        \n",
    "# FinalPageRankDF = NewPageRankDF_2.orderBy(F.desc(\"rank\")).select(\"id\", \"rank\")\n",
    "FinalPageRankDF = NewPageRankDF.orderBy(F.desc(\"rank\")).select(\"id\", \"rank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dabc89f7-94e0-4e57-bc53-9d7b08b38d75",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FinalPageRankDF.show()\n",
    "print(\"count\")\n",
    "print(count)\n",
    "print(\"temp\")\n",
    "print(temp)\n",
    "print(\"size\")\n",
    "print(size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45e0092c-c3da-49f3-932c-0d01333725a4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ReverseDF.filter(\"id == 19279158\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PageRank is one of the underlying algorithms used to rank pages on the internet and it consists of determining the relevance of a website with respect to other websites. This technique uses a directly proportional evaluation system between relevance and evaluation which implies that the higher a website is on the PageRank, the more relevant it is to the end-user. In this notebook, we proposed a possible rudimentary implementation of the PageRank algorithm. \n",
    "\n",
    "We first extract the data from the Wikipedia raw file and we extract the links that are on the page, making sure we don't obtain links to the same page in order to remove uni-page infinite cycles. Once we obtain the links in the page, we continue to the next page and extract its respective links and so on. This yields the Forward Links Table. From this table, we can calculate which links take me to the original site for each link in the table and effectively, construct a Reverse Links Table. Once we have both of these tables, we can then execute the PageRank algorithm iteratively. The algorithm changes the value of the rank using the formula above, and it stops when the data has reached a stable value. We attempted this by calculating a \"delta\" between iterations until the difference between the data was negligible. The end result of the algortihm returns a table with the pages order by rank. Using the small subset, we can confirm that the #1 ranked page also has one of the highest numbers of pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Clear_Version_Notebook",
   "notebookOrigID": 2148233874159061,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
